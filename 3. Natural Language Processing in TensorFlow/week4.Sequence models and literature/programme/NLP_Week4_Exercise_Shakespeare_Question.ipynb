{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Week4-Exercise-Shakespeare-Question.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOwsuGQQY9OL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "41814874-5b12-4c56-c833-4ba0f1e810bf"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRnDnCW-Z7qv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "084b6a21-0958-480c-a06f-b49af9f0114b"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
        "    -O /tmp/sonnets.txt\n",
        "data = open('/tmp/sonnets.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-24 21:24:35--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.126.128, 2a00:1450:4013:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.126.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93578 (91K) [text/plain]\n",
            "Saving to: ‘/tmp/sonnets.txt’\n",
            "\n",
            "/tmp/sonnets.txt    100%[===================>]  91.38K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-02-24 21:24:41 (142 MB/s) - ‘/tmp/sonnets.txt’ saved [93578/93578]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9vH8Y59ajYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "5d76d08a-a0c5-43fd-8d6e-309469aad798"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10, 100)           321100    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 10, 300)           301200    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 10, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1605)              162105    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3211)              5156866   \n",
            "=================================================================\n",
            "Total params: 6,101,671\n",
            "Trainable params: 6,101,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIg2f1HBxqof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8afc7c34-36da-4ae3-a890-bf2d3b8c8340"
      },
      "source": [
        " history = model.fit(predictors, label, epochs=100, verbose=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 15462 samples\n",
            "Epoch 1/100\n",
            "15462/15462 [==============================] - 23s 2ms/sample - loss: 6.9040 - acc: 0.0202\n",
            "Epoch 2/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 6.5005 - acc: 0.0207\n",
            "Epoch 3/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 6.3793 - acc: 0.0248\n",
            "Epoch 4/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 6.2578 - acc: 0.0317\n",
            "Epoch 5/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 6.1717 - acc: 0.0376\n",
            "Epoch 6/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 6.0896 - acc: 0.0396\n",
            "Epoch 7/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 6.0090 - acc: 0.0423\n",
            "Epoch 8/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 5.9117 - acc: 0.0484\n",
            "Epoch 9/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 5.7996 - acc: 0.0530\n",
            "Epoch 10/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 5.6825 - acc: 0.0612\n",
            "Epoch 11/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 5.5653 - acc: 0.0666\n",
            "Epoch 12/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 5.4493 - acc: 0.0732\n",
            "Epoch 13/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 5.3393 - acc: 0.0776\n",
            "Epoch 14/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 5.2295 - acc: 0.0854\n",
            "Epoch 15/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 5.1230 - acc: 0.0907\n",
            "Epoch 16/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 5.0168 - acc: 0.1013\n",
            "Epoch 17/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 4.9156 - acc: 0.1087\n",
            "Epoch 18/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 4.8164 - acc: 0.1176\n",
            "Epoch 19/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 4.7137 - acc: 0.1282\n",
            "Epoch 20/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 4.6041 - acc: 0.1352\n",
            "Epoch 21/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 4.5035 - acc: 0.1484\n",
            "Epoch 22/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 4.4030 - acc: 0.1608\n",
            "Epoch 23/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 4.3020 - acc: 0.1695\n",
            "Epoch 24/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 4.2068 - acc: 0.1793\n",
            "Epoch 25/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 4.0958 - acc: 0.1952\n",
            "Epoch 26/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 3.9994 - acc: 0.2081\n",
            "Epoch 27/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 3.8925 - acc: 0.2268\n",
            "Epoch 28/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 3.8055 - acc: 0.2445\n",
            "Epoch 29/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 3.7100 - acc: 0.2637\n",
            "Epoch 30/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 3.6107 - acc: 0.2773\n",
            "Epoch 31/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 3.5288 - acc: 0.2971\n",
            "Epoch 32/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 3.4321 - acc: 0.3177\n",
            "Epoch 33/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 3.3458 - acc: 0.3326\n",
            "Epoch 34/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 3.2598 - acc: 0.3520\n",
            "Epoch 35/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 3.1782 - acc: 0.3719\n",
            "Epoch 36/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 3.1081 - acc: 0.3882\n",
            "Epoch 37/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 3.0397 - acc: 0.4027\n",
            "Epoch 38/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 2.9626 - acc: 0.4208\n",
            "Epoch 39/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 2.8786 - acc: 0.4378\n",
            "Epoch 40/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 2.8101 - acc: 0.4577\n",
            "Epoch 41/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 2.7519 - acc: 0.4706\n",
            "Epoch 42/100\n",
            "15462/15462 [==============================] - 22s 1ms/sample - loss: 2.6934 - acc: 0.4860\n",
            "Epoch 43/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 2.6387 - acc: 0.4958\n",
            "Epoch 44/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 2.5631 - acc: 0.5124\n",
            "Epoch 45/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 2.5077 - acc: 0.5246\n",
            "Epoch 46/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 2.4591 - acc: 0.5329\n",
            "Epoch 47/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 2.4069 - acc: 0.5465\n",
            "Epoch 48/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 2.3689 - acc: 0.5563\n",
            "Epoch 49/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 2.3139 - acc: 0.5714\n",
            "Epoch 50/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 2.2627 - acc: 0.5807\n",
            "Epoch 51/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 2.2151 - acc: 0.5947\n",
            "Epoch 52/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 2.1706 - acc: 0.5993\n",
            "Epoch 53/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 2.1367 - acc: 0.6060\n",
            "Epoch 54/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 2.0907 - acc: 0.6224\n",
            "Epoch 55/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 2.0596 - acc: 0.6266\n",
            "Epoch 56/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 2.0254 - acc: 0.6348\n",
            "Epoch 57/100\n",
            "15462/15462 [==============================] - 22s 1ms/sample - loss: 1.9793 - acc: 0.6420\n",
            "Epoch 58/100\n",
            "15462/15462 [==============================] - 22s 1ms/sample - loss: 1.9426 - acc: 0.6522\n",
            "Epoch 59/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.9092 - acc: 0.6597\n",
            "Epoch 60/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.8831 - acc: 0.6631\n",
            "Epoch 61/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.8433 - acc: 0.6733\n",
            "Epoch 62/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.8062 - acc: 0.6816\n",
            "Epoch 63/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.7781 - acc: 0.6852\n",
            "Epoch 64/100\n",
            "15462/15462 [==============================] - 22s 1ms/sample - loss: 1.7588 - acc: 0.6899\n",
            "Epoch 65/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.7285 - acc: 0.6931\n",
            "Epoch 66/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.7069 - acc: 0.6989\n",
            "Epoch 67/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.6742 - acc: 0.7077\n",
            "Epoch 68/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.6471 - acc: 0.7153\n",
            "Epoch 69/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.6330 - acc: 0.7159\n",
            "Epoch 70/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.6060 - acc: 0.7195\n",
            "Epoch 71/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.5796 - acc: 0.7251\n",
            "Epoch 72/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.5660 - acc: 0.7276\n",
            "Epoch 73/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.5366 - acc: 0.7329\n",
            "Epoch 74/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.5219 - acc: 0.7332\n",
            "Epoch 75/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.4843 - acc: 0.7421\n",
            "Epoch 76/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.4668 - acc: 0.7469\n",
            "Epoch 77/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.4579 - acc: 0.7494\n",
            "Epoch 78/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.4482 - acc: 0.7489\n",
            "Epoch 79/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.4156 - acc: 0.7582\n",
            "Epoch 80/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.3964 - acc: 0.7593\n",
            "Epoch 81/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.3910 - acc: 0.7581\n",
            "Epoch 82/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.3787 - acc: 0.7629\n",
            "Epoch 83/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.3553 - acc: 0.7653\n",
            "Epoch 84/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.3429 - acc: 0.7683\n",
            "Epoch 85/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.3230 - acc: 0.7726\n",
            "Epoch 86/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.3205 - acc: 0.7712\n",
            "Epoch 87/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.2855 - acc: 0.7784\n",
            "Epoch 88/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.2777 - acc: 0.7827\n",
            "Epoch 89/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.2776 - acc: 0.7792\n",
            "Epoch 90/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.2575 - acc: 0.7825\n",
            "Epoch 91/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.2413 - acc: 0.7873\n",
            "Epoch 92/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.2199 - acc: 0.7922\n",
            "Epoch 93/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.2178 - acc: 0.7881\n",
            "Epoch 94/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.2198 - acc: 0.7918\n",
            "Epoch 95/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.1938 - acc: 0.7930\n",
            "Epoch 96/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.1879 - acc: 0.7970\n",
            "Epoch 97/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.1770 - acc: 0.7960\n",
            "Epoch 98/100\n",
            "15462/15462 [==============================] - 20s 1ms/sample - loss: 1.1750 - acc: 0.7957\n",
            "Epoch 99/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.1606 - acc: 0.7989\n",
            "Epoch 100/100\n",
            "15462/15462 [==============================] - 21s 1ms/sample - loss: 1.1417 - acc: 0.8027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fXTEO3GJ282",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vc6PHgxa6Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}